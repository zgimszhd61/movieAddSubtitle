1
00:00:00,000 --> 00:00:02,760
the training of the model and making it really large.

2
00:00:02,760 --> 00:00:04,880
So I think scale will be the primary determining factor.

3
00:00:04,880 --> 00:00:07,240
It's like the first principle component of things, for sure.

4
00:00:07,240 --> 00:00:12,440
But there are many of the other things that you need to get right.

5
00:00:12,440 --> 00:00:16,120
So it's almost like the scale sets some kind of a speed limit almost.

6
00:00:16,120 --> 00:00:18,000
But you do need some of the other things,

7
00:00:18,000 --> 00:00:19,440
but it's like if you don't have the scale,

8
00:00:19,440 --> 00:00:22,240
then you fundamentally just can't train some of these massive models

9
00:00:22,240 --> 00:00:24,000
if you are going to be training models.

10
00:00:24,000 --> 00:00:25,880
If you're just going to be doing fine tuning and so on,

11
00:00:25,880 --> 00:00:29,560
then I think maybe less scale is necessary.

12
00:00:29,560 --> 00:00:33,000
But we haven't really seen that just yet to fully play out.

13
00:00:33,000 --> 00:00:34,600
And can you share more about some of the ingredients

14
00:00:34,600 --> 00:00:35,600
that you think also matter,

15
00:00:35,600 --> 00:00:40,200
maybe lower in priority behind scale?

16
00:00:40,200 --> 00:00:43,960
Yeah, so the first thing I think is like

17
00:00:43,960 --> 00:00:45,040
you can't just train these models.

18
00:00:45,040 --> 00:00:47,360
If you're just given the money and the scale,

19
00:00:47,360 --> 00:00:49,240
it's actually still really hard to build these models.

20
00:00:49,240 --> 00:00:51,600
And part of it is that the infrastructure is still so new

21
00:00:51,600 --> 00:00:53,240
and it's still being developed and not quite there.

22
00:00:53,240 --> 00:00:56,160
But training these models at scale is extremely difficult

23
00:00:56,160 --> 00:00:59,440
and is a very complicated distributed optimization problem.

24
00:00:59,440 --> 00:01:01,080
And there's actually like the talent for this

25
00:01:01,080 --> 00:01:02,840
is fairly scarce right now.

26
00:01:02,840 --> 00:01:06,560
And it just basically turns into this insane thing

27
00:01:06,560 --> 00:01:08,360
running on tens of thousands of GPUs.

28
00:01:08,360 --> 00:01:09,840
All of them are like failing at random

29
00:01:09,840 --> 00:01:10,800
at different points in time.

30
00:01:10,800 --> 00:01:12,920
And so like instrumenting that and getting that to work

31
00:01:12,920 --> 00:01:15,480
is actually an extremely difficult challenge.

32
00:01:15,480 --> 00:01:19,360
GPUs were not like intended for like 10,000 GPU workloads

33
00:01:19,360 --> 00:01:20,680
until very recently.

34
00:01:20,680 --> 00:01:24,040
And so I think a lot of the infrastructure is sort of like

35
00:01:24,040 --> 00:01:25,640
creaking under that pressure.

36
00:01:25,640 --> 00:01:27,360
And we need to like work through that.

37
00:01:27,360 --> 00:01:29,120
But right now, if you're just giving someone a ton of money

38
00:01:29,120 --> 00:01:30,920
or a ton of scale or GPUs, it's not obvious to me

39
00:01:30,920 --> 00:01:32,440
that they can just produce one of these models,

40
00:01:32,440 --> 00:01:35,840
which is why it's not just about scale.

41
00:01:35,840 --> 00:01:37,960
You actually need a ton of expertise,

42
00:01:37,960 --> 00:01:41,360
both on the infrastructure side, the algorithms side,

43
00:01:41,360 --> 00:01:43,680
and then the data side and being careful with that.

44
00:01:43,680 --> 00:01:46,280
So I think those are the major components.

45
00:01:46,280 --> 00:01:48,760
The ecosystem is moving so quickly.

46
00:01:48,760 --> 00:01:51,480
Even some of the challenges we thought existed a year ago

47
00:01:51,480 --> 00:01:53,960
are being solved more and more today.

48
00:01:53,960 --> 00:01:57,760
Hallucinations, context windows, multimodal capabilities,

49
00:01:57,760 --> 00:01:59,880
inference getting better, faster, cheaper.

50
00:02:01,200 --> 00:02:04,920
What are the LLM research challenges today

51
00:02:04,920 --> 00:02:06,040
that keep you up at night?

52
00:02:06,040 --> 00:02:08,280
What do you think are immediate enough problems

53
00:02:08,280 --> 00:02:11,520
but also solvable problems that we can continue to go after?

54
00:02:13,360 --> 00:02:14,520
So I would say on the algorithm side,

55
00:02:14,520 --> 00:02:16,640
one thing I'm thinking about quite a bit is

56
00:02:16,640 --> 00:02:19,960
this like distinct split between diffusion models

57
00:02:19,960 --> 00:02:21,120
and autoregressive models.

58
00:02:21,120 --> 00:02:22,320
They're both ways of presenting

59
00:02:22,320 --> 00:02:23,440
probability distributions.

60
00:02:23,440 --> 00:02:25,040
And it just turns out that different modalities

61
00:02:25,040 --> 00:02:28,080
are apparently a good fit for one of the two.

62
00:02:28,080 --> 00:02:30,640
I think that there's probably some space to unify them

63
00:02:30,640 --> 00:02:33,240
or to like connect them in some way

64
00:02:33,240 --> 00:02:37,360
and also get some best of both worlds

65
00:02:37,360 --> 00:02:38,840
or sort of figure out

66
00:02:38,840 --> 00:02:41,320
how we can get a hybrid architecture and so on.

67
00:02:41,320 --> 00:02:43,240
So it's just odd to me that we have sort of like

68
00:02:43,240 --> 00:02:46,000
two separate points in the space of models

69
00:02:46,000 --> 00:02:47,560
and they're both extremely good.

70
00:02:47,560 --> 00:02:48,880
And it just feels wrong to me

71
00:02:48,880 --> 00:02:50,360
that there's nothing in between.

72
00:02:50,360 --> 00:02:52,000
So I think we'll see that sort of carved out

73
00:02:52,000 --> 00:02:54,080
and I think there are interesting problems there.

74
00:02:54,120 --> 00:02:56,520
And then the other thing that maybe I would point to is

75
00:02:57,600 --> 00:02:59,480
there's still like a massive gap

76
00:02:59,480 --> 00:03:04,080
in just the energetic efficiency of running all this stuff.

77
00:03:04,080 --> 00:03:06,640
So my brain is 20 Watts roughly.

78
00:03:06,640 --> 00:03:08,600
Jensen was just talking at GTC about

79
00:03:08,600 --> 00:03:10,080
the massive super computers

80
00:03:10,080 --> 00:03:11,120
that they're gonna be building now.

81
00:03:11,120 --> 00:03:15,000
These are, the numbers are in megawatts, right?

82
00:03:15,000 --> 00:03:17,040
And so maybe you don't need all that to run like a brain.

83
00:03:17,040 --> 00:03:18,800
I don't know how much you need exactly,

84
00:03:18,800 --> 00:03:20,760
but I think it's safe to say we're probably off

85
00:03:20,760 --> 00:03:23,200
by a factor of 1,000 to like a million somewhere there

86
00:03:23,200 --> 00:03:26,480
in terms of the efficiency of running these models.

87
00:03:26,480 --> 00:03:28,280
And I think part of it is just because the computers

88
00:03:28,280 --> 00:03:30,480
we've designed of course are just like not a good fit

89
00:03:30,480 --> 00:03:32,240
for this workload.

90
00:03:32,240 --> 00:03:35,240
And I think Nvidia GPUs are like a good step

91
00:03:35,240 --> 00:03:36,200
in that direction.

92
00:03:37,560 --> 00:03:40,320
In terms of like the, you need extremely high parallelism.

93
00:03:40,320 --> 00:03:42,040
We don't actually care about sequential computation

94
00:03:42,040 --> 00:03:44,160
that is sort of like data dependent in some way.

95
00:03:44,160 --> 00:03:47,160
We just have these, we just need to like blast

96
00:03:47,160 --> 00:03:50,720
the same algorithm across many different sort of

97
00:03:50,720 --> 00:03:53,040
array elements or something you can think about it that way.

98
00:03:53,040 --> 00:03:55,520
So I would say number one is just adapting

99
00:03:55,520 --> 00:03:58,600
the computer architecture to the new data workflows.

100
00:03:58,600 --> 00:04:00,080
Number two is like pushing on a few things

101
00:04:00,080 --> 00:04:02,000
that we're currently seeing improvements on.

102
00:04:02,000 --> 00:04:03,960
So number one maybe is precision.

103
00:04:03,960 --> 00:04:06,240
We're seeing precision come down from what originally was

104
00:04:06,240 --> 00:04:08,520
was like 64 bit for double.

105
00:04:08,520 --> 00:04:11,720
We're now down to, I don't know what it is, 456

106
00:04:11,720 --> 00:04:14,240
or even 1.58 depending on which papers you read.

107
00:04:14,240 --> 00:04:16,280
And so I think precision is one big lever

108
00:04:16,280 --> 00:04:18,500
of getting a handle on this.

109
00:04:18,500 --> 00:04:20,720
And then second one of course is sparsity.

110
00:04:20,720 --> 00:04:22,720
So that's also like another big delta I would say,

111
00:04:22,720 --> 00:04:24,760
like your brain is not always fully activated.

112
00:04:24,760 --> 00:04:27,120
And so sparsity I think is another big lever.

113
00:04:27,120 --> 00:04:28,480
But then the last lever I also feel like

114
00:04:28,480 --> 00:04:30,560
just the one moment architecture of like computers

115
00:04:30,560 --> 00:04:32,440
and how they build where you're shuttling data in and out

116
00:04:32,440 --> 00:04:34,080
and doing a ton of data movement between memory

117
00:04:34,080 --> 00:04:36,160
and the cores that are doing all the compute.

118
00:04:36,160 --> 00:04:37,640
This is all broken as well kind of

119
00:04:37,640 --> 00:04:38,640
and it's not how your brain works

120
00:04:38,640 --> 00:04:39,800
and that's why it's so efficient.

121
00:04:39,800 --> 00:04:41,800
And so I think it should be a very exciting time

122
00:04:41,800 --> 00:04:42,800
in computer architecture.

123
00:04:42,800 --> 00:04:43,840
I'm not a computer architect,

124
00:04:43,840 --> 00:04:46,160
but I think there's, it seems like we're off

125
00:04:46,160 --> 00:04:48,040
by a factor of million, thousand to a million

126
00:04:48,040 --> 00:04:48,880
something like that.

127
00:04:48,880 --> 00:04:53,320
And there should be really exciting sort of innovations there

128
00:04:53,320 --> 00:04:55,680
that bring that down.

